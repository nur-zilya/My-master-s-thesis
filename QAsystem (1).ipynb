{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq_YPD73xTXR"
      },
      "outputs": [],
      "source": [
        "!pip uninstall spacy -y\n",
        "!pip install -U spacy>=3.0\n",
        "!python -m spacy download ru_core_news_md\n",
        "import spacy\n",
        "nlp = spacy.load('ru_core_news_md')\n",
        "nlp.max_length = 10000000\n",
        "import nltk\n",
        "nltk.download ('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAOi-g451oSJ",
        "outputId": "e5485d11-100b-49cd-e99a-ce7288d04eee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chardet\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/drive/MyDrive/Thesis/data/texts\"):\n",
        "    for filename in files:\n",
        "        path = f'/content/drive/MyDrive/Thesis/data/texts/{filename}'\n",
        "\n",
        "        with open(path, 'rb') as file:\n",
        "            raw_data = file.read()\n",
        "            result = chardet.detect(raw_data)\n",
        "            encoding = result['encoding']\n",
        "\n",
        "        with open(path, encoding=encoding) as file:\n",
        "            text = file.read()\n",
        "\n",
        "        document = nlp(text)\n",
        "        outname = path.replace('.txt', '-lemmatized.txt')\n",
        "        with open(outname, 'w', encoding='utf-8') as out:\n",
        "            for token in document:\n",
        "                out.write(token.lemma_.lower())\n",
        "                out.write(' ')\n"
      ],
      "metadata": {
        "id": "JuxE10tY2MGF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chardet\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "all_tokens = []\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/drive/MyDrive/Thesis/data/texts/lemmas\"):\n",
        "    for filename in files:\n",
        "        lpath = f'/content/drive/MyDrive/Thesis/data/texts/lemmas/{filename}'\n",
        "        with open(lpath, 'rb') as f:\n",
        "            raw_data = f.read()\n",
        "            result = chardet.detect(raw_data)\n",
        "            encoding = result['encoding']\n",
        "        text_lem = open(lpath, encoding=encoding).read()\n",
        "        tokens = nltk.word_tokenize(text_lem)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "sentences = [nltk.word_tokenize(text) for text in all_tokens]\n",
        "\n",
        "model = Word2Vec(sentences, min_count=1)\n"
      ],
      "metadata": {
        "id": "lUj2KPIRB66q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model.wv\n",
        "all_word_vectors = word_vectors.vectors\n",
        "\n",
        "print(\"Размерность векторов:\", all_word_vectors.shape)"
      ],
      "metadata": {
        "id": "UnZfyLvSB4qU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}